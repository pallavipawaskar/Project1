# -*- coding: utf-8 -*-
"""Credit EDA assignment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOZnAdLDphNuKFNe_4Qzw547yh8-pfJq

Identifying patterns of financial behaviour of clients regarding repayment of loan. Understand the driving factors behind loan default.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#loading files
app_data=pd.read_csv("/content/application_data.csv")
col_desc=pd.read_csv("/content/columns_description.csv")
prev_app=pd.read_csv("/content/previous_application.csv")

#load files in dataframe
df_app=pd.DataFrame(app_data)
df_prev=pd.DataFrame(prev_app)
df_col=pd.DataFrame(col_desc)

"""Analysing 1st dataset"""

#reading files
df_app.head()

#checking shape of dataframe
df_app.shape

#checking information of dataframe
df_app.info()

#getting information about duplicates
df_app.duplicated().sum()

#cheking percentage of missing values in each column
df_app.isnull().mean()

#dropping some columns as they provide no value to dataset
df_app.drop(["FLAG_DOCUMENT_18","FLAG_DOCUMENT_19","FLAG_DOCUMENT_20","FLAG_DOCUMENT_21","FLAG_DOCUMENT_14","FLAG_DOCUMENT_15","FLAG_DOCUMENT_16","FLAG_DOCUMENT_17"],axis=1,inplace=True)

#analysing change in dataframe
df_app.head()

#dropping columns which are less useful and disturbs our analysis
df_app.drop(["EMERGENCYSTATE_MODE","OBS_30_CNT_SOCIAL_CIRCLE","DEF_30_CNT_SOCIAL_CIRCLE","OBS_60_CNT_SOCIAL_CIRCLE","DEF_60_CNT_SOCIAL_CIRCLE"],axis=1,inplace=True)

df_app.drop(["FONDKAPREMONT_MODE","HOUSETYPE_MODE","WALLSMATERIAL_MODE"],axis=1,inplace=True)

df_app.drop(["LIVINGAREA_MEDI","NONLIVINGAPARTMENTS_MEDI","NONLIVINGAREA_MEDI"],axis=1,inplace=True)

df_app.drop(["FLOORSMIN_MEDI","LANDAREA_MEDI","LIVINGAPARTMENTS_MEDI"],axis=1,inplace=True)

df_app.drop(["TARGET","ELEVATORS_MEDI","ENTRANCES_MEDI","FLOORSMAX_MEDI","TOTALAREA_MODE"],axis=1,inplace=True)

df_app.drop(["FLAG_DOCUMENT_2","FLAG_DOCUMENT_3","FLAG_DOCUMENT_4","FLAG_DOCUMENT_5","FLAG_DOCUMENT_6","FLAG_DOCUMENT_7","FLAG_DOCUMENT_8","FLAG_DOCUMENT_9","FLAG_DOCUMENT_10","FLAG_DOCUMENT_11","FLAG_DOCUMENT_12","FLAG_DOCUMENT_13","AMT_REQ_CREDIT_BUREAU_HOUR","AMT_REQ_CREDIT_BUREAU_DAY","AMT_REQ_CREDIT_BUREAU_WEEK","AMT_REQ_CREDIT_BUREAU_MON","AMT_REQ_CREDIT_BUREAU_QRT","AMT_REQ_CREDIT_BUREAU_YEAR"],axis=1,inplace=True)

#dropping columns having around 50% missing values
df_app=df_app.loc[:, df_app.isnull().mean() < 0.48]

#analysing change in dataframe
df_app.head()

#checking null values
df_app.isnull().mean()

df_app.value_counts()

#dropping columns which provide less value
df_app.drop(["REG_REGION_NOT_LIVE_REGION","REG_REGION_NOT_WORK_REGION","LIVE_REGION_NOT_WORK_REGION","REG_CITY_NOT_LIVE_CITY","REG_CITY_NOT_WORK_CITY","LIVE_CITY_NOT_WORK_CITY"],axis=1,inplace=True)

#reorganising data
df_app["NAME_EDUCATION_TYPE"].replace(["Higher education","Secondary / secondary special"],["Higher education","Secondary"],inplace=True)

df_app["NAME_FAMILY_STATUS"].replace(["Married","Single / not married"],["Married","Single"],inplace=True)

df_app["NAME_HOUSING_TYPE"].replace(["Rented apartment","House / apartment"],["Rented","Own"],inplace=True)

#checking dataset
df_app.isnull().sum()

#imputing NaN values of column by Unknown
df_app["OCCUPATION_TYPE"].replace(np.nan,"Unknown",inplace=True)

df_app["EXT_SOURCE_3"].replace(np.nan,df_app["EXT_SOURCE_3"].mean(),inplace=True)

#checking particular columns to imputing values
df_app["EXT_SOURCE_3"].mean()

df_app["EXT_SOURCE_2"].replace(np.nan,df_app["EXT_SOURCE_2"].mean(),inplace=True)

df_app["NAME_TYPE_SUITE"].replace(np.nan,"Unknown",inplace=True)

df_app['AMT_ANNUITY'].describe()

df_app['AMT_ANNUITY'].replace(np.nan,df_app['AMT_ANNUITY'].median(),inplace=True)

df_app['AMT_GOODS_PRICE'].replace(np.nan,df_app['AMT_GOODS_PRICE'].median(),inplace=True)

df_app["CNT_FAM_MEMBERS"].describe()

df_app["CNT_FAM_MEMBERS"].fillna(df_app["CNT_FAM_MEMBERS"].median(),inplace=True)

df_app["DAYS_LAST_PHONE_CHANGE"].fillna(df_app["DAYS_LAST_PHONE_CHANGE"].median(),inplace=True)

#viewing cleaned Dataset
df_app.isnull().sum()

df_app['AMT_INCOME_TOTAL'] = df_app['AMT_INCOME_TOTAL'].apply(lambda x: f"{x:,.0f}")

df_app['AMT_INCOME_TOTAL'].describe()

#data formating and new required columns
numeric_cols = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',
                'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START']

for col in numeric_cols:
    df_app[col] = pd.to_numeric(df_app[col], errors='coerce')
df_prev['DAYS_DECISION'] = pd.to_numeric(df_prev['DAYS_DECISION'], errors='coerce')
df_app['annuity_income_ratio'] = df_app['AMT_ANNUITY'] / df_app['AMT_INCOME_TOTAL']
df_app['goods_credit_ratio']   = df_app['AMT_GOODS_PRICE'] / df_app['AMT_CREDIT']
df_app['years_employed']       = df_app['DAYS_EMPLOYED'] / -365
df_app['age_years']            = df_app['DAYS_BIRTH'] / -365
df_app['credit_per_family_member'] = df_app['AMT_CREDIT'] / df_app['CNT_FAM_MEMBERS']
df_app['ext_source_avg']       = df_app[['EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)

df_app.head()

df_app.shape

"""Analysing 2nd dataset"""

#Analysing 2nd dataset of column description
df_col.describe()

df_col.info()

df_col.nunique()

df_col.duplicated().sum()

# analysis of df_col
df_col.isnull().mean()

#Dropping column Special as it is not adding any value and also consist of 56% missing values
df_col.drop("Special",axis=1,inplace=True)

#Table column can also be dropped as it is not adding any value to dataset
df_col.drop("Table",axis=1,inplace=True)

#checking dataset
df_col.isnull().sum()

#clean dataset about column data
df_col.head()

"""Analysing 3rd dataset of previous application"""

#analysing 3rd dataset of previous application
df_prev.head()

df_prev.shape

df_prev.info()

df_prev.describe()

#data formating on columns
df_prev["NAME_CONTRACT_TYPE"].replace(["Cash loans","Revolving loans"],["Cash","Revolving"],inplace=True)

for col in ['CNT_PAYMENT','SK_ID_PREV', 'SK_ID_CURR',"AMT_ANNUITY","AMT_APPLICATION","AMT_CREDIT","AMT_DOWN_PAYMENT","AMT_GOODS_PRICE","HOUR_APPR_PROCESS_START","NFLAG_LAST_APPL_IN_DAY"]:
    df_prev[col] = df_prev[col].apply(lambda x: f"{x:,.0f}")

df_prev.head()

df_prev.isnull().mean()

#dropping column as it has around 50% missing values and adds no value to dataset
df_prev.drop(["RATE_INTEREST_PRIVILEGED","RATE_INTEREST_PRIMARY","NFLAG_INSURED_ON_APPROVAL"],axis=1,inplace=True)

df_prev.drop(["RATE_DOWN_PAYMENT"],axis=1,inplace=True)

df_prev["NAME_TYPE_SUITE"].replace(np.nan,"Unknown",inplace=True)

df_prev["CNT_PAYMENT"].head()

#checking null values in datasets again
df_prev.isnull().sum()

df_prev["DAYS_FIRST_DRAWING"].head()

df_prev["DAYS_FIRST_DUE"].head()

#around 50% values in important columns are missing thus droping all NaN
df_prev.dropna(inplace=True)

df_prev.shape

#cleaned Dataset previous_application
df_prev.head()

"""Outliners"""

#checking outliners
for col in ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']:
    Q1 = df_app[col].quantile(0.25)
    Q3 = df_app[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df_app[(df_app[col] < Q1 - 1.5 * IQR) | (df_app[col] > Q3 + 1.5 * IQR)]
    print(f"{col} â†’ {len(outliers)} outliers")

cols_to_plot = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']
#plotting in loop
for col in cols_to_plot:
    plt.figure(figsize=(20, 10))
    sns.boxplot(x=df_app[col], color='red')
    plt.title(f'Distribution and Outliers in {col}')
    plt.xlabel(col)
    plt.show()

"""Univariate, bivariate analysis for 1st dataset"""

#univariate analysis of application dataset
df_app[['AMT_CREDIT', 'AMT_ANNUITY', 'AMT_INCOME_TOTAL']].hist(bins=30, figsize=(20, 10))

df_app['NAME_CONTRACT_TYPE'].value_counts().plot(kind='bar', title='Contract Type Frequency')

df_app.columns

#bivariate analysis
#Dataset Application data
sns.boxplot(data=df_app, x='NAME_EDUCATION_TYPE', y='AMT_CREDIT')
plt.title("Credit Amount by Education Type")

corr_matrix = df_app.select_dtypes(include='number').corr()
corr_pairs = corr_matrix.abs().unstack().sort_values(ascending=False)
corr_pairs = corr_pairs[corr_pairs < 1.0].drop_duplicates()
top_corr = corr_pairs.head(10)
print("correlated variable pairs:\n", top_corr)

subset_cols = ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_ANNUITY',
               'CNT_CHILDREN', 'DAYS_EMPLOYED', 'EXT_SOURCE_2', 'EXT_SOURCE_3']

corr_subset = df_app[subset_cols].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_subset, annot=True, fmt=".2f", cmap='mako')
plt.title("Heatmap of Selected Features")
plt.tight_layout()
plt.show()

"""Univariate, bivariate analysis for 3rd dataframe"""

df_prev.columns

#univariate analysis
numeric_cols = df_prev.select_dtypes(include=['int64', 'float64']).columns

for col in numeric_cols:
    plt.figure(figsize=(8, 3))
    sns.histplot(df_prev[col], bins=30, kde=True, color='teal')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

categorical_cols = df_prev.select_dtypes(include='object').columns

for col in categorical_cols:
    plt.figure(figsize=(8, 3))
    df_prev[col].value_counts().plot(kind='bar', color='orchid')
    plt.title(f'Frequency of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

#Bivariate analysis
for cat_col in ['NAME_CONTRACT_TYPE', 'NAME_CASH_LOAN_PURPOSE', 'PRODUCT_COMBINATION']:
    for num_col in ['AMT_CREDIT', 'AMT_ANNUITY', 'CNT_PAYMENT']:
        plt.figure(figsize=(8, 4))
        sns.boxplot(data=df_prev, x=cat_col, y=num_col)
        plt.title(f'{num_col} by {cat_col}')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

sns.scatterplot(data=df_prev, x='AMT_APPLICATION', y='AMT_CREDIT', alpha=0.6)
plt.title("AMT_CREDIT vs AMT_APPLICATION")
plt.tight_layout()
plt.show()

sns.scatterplot(data=df_prev, x='AMT_ANNUITY', y='CNT_PAYMENT', alpha=0.6)
plt.title("AMT_ANNUITY vs CNT_PAYMENT")
plt.tight_layout()
plt.show()

#correlation
corr = df_prev[numeric_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", center=0)
plt.title("Correlation Matrix of Numeric Features")
plt.tight_layout()
plt.show()